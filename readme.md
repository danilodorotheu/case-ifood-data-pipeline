# Case iFood - Data Lake & Data Pipeline

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o de um fluxo automatizado de aquisiÃ§Ã£o, ingestÃ£o e processamento de dados pÃºblicos de corridas de tÃ¡xi da cidade de Nova York, utilizando **AWS Lambda**, **AWS Glue** e **Amazon S3**.  

---

## ğŸ¯ Proposta do Case

Criar um **workflow de processamento automatizado** que:

1. Baixa os arquivos pÃºblicos de corridas de tÃ¡xi de Nova York em um diretÃ³rio de staging (**Landing**).  
2. IngestÃ£o os arquivos na camada **SoR**.  
3. Cria uma visÃ£o tratada de corridas (**rides**) na camada **SoT**.  
4. Disponibiliza os dados para consultas SQL via **Athena** e para uso em **SageMaker** ou **Glue Jobs**.

> ğŸ” Apesar de a camada **Spec** existir na arquitetura, ela **nÃ£o serÃ¡ utilizada neste case**.


---

## ğŸ“Œ Estrutura do RepositÃ³rio
case-ifood
â”œâ”€â”€ Analysis
â”‚   â””â”€â”€ perguntas.sql
â””â”€â”€ src
    â”œâ”€â”€ case-ifood-app-lambda-get
    â”œâ”€â”€ case-ifood-app-lambda-ingest
    â”œâ”€â”€ case-ifood-app-glue-tbsor_yellow_tripdata
    â”‚   â””â”€â”€ scripts
    â”‚       â””â”€â”€ glue_job_yellow.py
    â”œâ”€â”€ case-ifood-infra-event-call-tbsot_yellow_rides
    â”œâ”€â”€ case-ifood-infra-setup
    â””â”€â”€ governed
        â”œâ”€â”€ sor
        â”‚   â””â”€â”€ case-ifood-infra-setup
        â”œâ”€â”€ sot
        â”‚   â””â”€â”€ case-ifood-infra-setup
        â””â”€â”€ spec

---

## âš™ï¸ Componentes

### ğŸ”¹ `case-ifood-infra-setup`
Infraestrutura inicial do Data Lake:
- CriaÃ§Ã£o dos buckets no **S3**.
- OrganizaÃ§Ã£o de diretÃ³rios (`landing`, `sor`, `sot`, `spec`).
- CriaÃ§Ã£o de tabelas Glue Catalog.

OBS: Neste local, seguem as modelagens (na pasta 'governed') das tabelas utilizadas no case:
- tbsor_yellow_tripdata
- tbsot_yellow_rides

---

### ğŸ”¹ `case-ifood-app-lambda-get`
FunÃ§Ã£o Lambda responsÃ¡vel por:
- Fazer **download dos arquivos de origem** ([NYC Taxi Trip Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)).
- Pousar os arquivos brutos no bucket **landing-zone**.

---

### ğŸ”¹ `case-ifood-app-lambda-ingest`
FunÃ§Ã£o Lambda responsÃ¡vel por:
- Ler os arquivos da camada **landing**.
- Carregar os dados brutos no database **tbsor_yellow_tripdata** (SoR).

---

### ğŸ”¹ `case-ifood-infra-event-call-tbsor_yellow_tripdata`
ConfiguraÃ§Ã£o de evento no **S3**:
- Detecta quando um novo arquivo chega na **landing-zone**.
- Dispara automaticamente a execuÃ§Ã£o da **lambda-ingest**.

---

### ğŸ”¹ `case-ifood-app-glue-tbsor_yellow_tripdata`
Job do **AWS Glue** responsÃ¡vel por:
- Ler os dados da camada **SoR**.
- Aplicar transformaÃ§Ãµes, filtros e seleÃ§Ã£o de colunas relevantes.
- Gravar o resultado na tabela **tbsot_yellow_rides** (SoT).

---

## ğŸ—ï¸ Arquitetura

A arquitetura da soluÃ§Ã£o segue um **workflow de ingestÃ£o e transformaÃ§Ã£o de dados** em mÃºltiplas camadas, conforme o desenho abaixo:

![Arquitetura](docs/arquitetura.jpg)

1. **AquisiÃ§Ã£o de Dados (Lambda GET)**  
   Busca o dado de origem e pousa na **landing-zone** (S3).

2. **IngestÃ£o (Lambda INGEST)**  
   Movimenta os dados da camada **Landing** para a camada **SoR**.

3. **Processamento (Glue Job)**  
   LÃª os dados brutos da **SoR**, aplica tratamentos e gera a visÃ£o tratada em **SoT**.

4. **Camada Spec (opcional)**  
   Conceitualmente prevista para processamentos especializados, mas **nÃ£o utilizada neste case**.

5. **Consumidores**  
   Dados da SoT podem ser consumidos por:
   - **Athena** (SQL Serverless para anÃ¡lise).
   - **SageMaker** (Machine Learning).
   - **Glue Jobs** (pipelines adicionais).

---

## ğŸ—‚ï¸ Contexto das Camadas

- **SoR (System of Record)**  
  Camada de registro bruto, contendo os dados exatamente como recebidos da origem, sem alteraÃ§Ãµes.  
  Exemplo: `tbsor_yellow_tripdata`.

- **SoT (System of Truth)**  
  Camada de verdade Ãºnica, com dados tratados, padronizados e prontos para consumo corporativo.  
  Exemplo: `tbsot_yellow_rides`.

- **Spec (Specialized Processing Engines)**  
  Camada opcional destinada a processamentos especializados (modelos, cÃ¡lculos especÃ­ficos, mÃ©tricas de negÃ³cio).  
  Neste **case**, a Spec estÃ¡ presente apenas conceitualmente.

---

## ğŸš€ PrÃ³ximos Passos

- Habilitar camada **Spec** para cenÃ¡rios avanÃ§ados (machine learning, cÃ¡lculos customizados).  
- Integrar com **pipelines de consumo real-time** (Kinesis).  
- Criar **monitoraÃ§Ã£o e alertas** (CloudWatch + SNS).  

---
